{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86d342da",
   "metadata": {},
   "source": [
    "# Introduction to Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd44a30",
   "metadata": {},
   "source": [
    "A Gaussian process is a probability distribution over possible functions that fit a set of data points.\n",
    "\n",
    "In Gaussian process regression (GPR), the concept of a prior and posterior distribution is used to make predictions about the function that generated the data. The prior distribution is the initial belief about the function before any data is observed, and the posterior distribution is the updated belief about the function after the data is observed.\n",
    "\n",
    "The prior distribution is defined by the mean function and covariance function (also known as the kernel) of the Gaussian process. These parameters can be specified by the user, or they can be estimated from the data. The posterior distribution is then computed using Bayesian inference, based on the observed data and the prior distribution.\n",
    "\n",
    "The posterior distribution represents the updated belief about the function based on the observed data, and it can be used to make predictions about the function at new input points. The predictions are obtained by sampling from the posterior distribution, which gives a set of possible functions that could have generated the observed data. The mean of these functions can be used as the predicted output value, and the variance of the functions can be used to compute the uncertainty of the predictions.\n",
    "\n",
    "The core definition of a Gaussian process states that for any finite set of input points $\\boldsymbol{x} = \\left\\{ x_1, x_2, ..., x_n \\right\\}$, the corresponding function values $f(\\boldsymbol{x}) = \\left\\{ f(x_1), f(x_2), ..., f(x_n) \\right\\}$ follow a multivariate Gaussian distribution. \n",
    "\n",
    "This distribution is characterized by a mean vector $\\boldsymbol{\\mu}$, where $\\mu_i = m(x_i)$ with $m(\\boldsymbol{x})$ being the mean function, \n",
    "and a covariance matrix $\\boldsymbol{K}$, where $K_{ij} = k(x_i,x_j)$ with $k(\\boldsymbol{x},\\boldsymbol{x'})$ being the covariance or kernel function: \n",
    "\n",
    "$ f(x) \\sim \\mathcal{GP} \\left( m(\\boldsymbol{x}), k(\\boldsymbol{x},\\boldsymbol{x'}) \\right) $,\n",
    "\n",
    "we usually take the mean equal to zero, although it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from make_a_gif import gif\n",
    "sns.set_style(\n",
    "    style='darkgrid', \n",
    "    rc={'axes.facecolor': '.95', 'grid.color': '.85'}\n",
    ")\n",
    "sns.set_palette(palette='deep')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ecca3d",
   "metadata": {},
   "source": [
    "## Target Function and Data Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcbd58d",
   "metadata": {},
   "source": [
    "We consider \n",
    "\n",
    "$ y = f(x)+ \\epsilon $\n",
    "\n",
    "being noisy observations of a target (true) function $f(x)$, where the residual errors $\\epsilon$ are modeled as white noise meaning they are normally, independently and identically distributed with zero mean: \n",
    "\n",
    "$ \\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2) $. \n",
    "\n",
    "As an example, let us consider function:\n",
    "\n",
    "$ f(x) = \\sin(2 \\pi x) + \\sin(4 \\pi x) + \\sin(7 \\pi x) + x $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d160c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def f(x):\n",
    "    f = np.sin(2 * np.pi * x) + np.sin(4 * np.pi * x) + np.sin(7 * np.pi * x) + x\n",
    "    return f\n",
    "\n",
    "# Generate 500 evenly-spaced x values in [0,1]\n",
    "x = np.linspace(start=0, stop=1, num=500)\n",
    "\n",
    "# Evaluate the function at generated points\n",
    "fx = f(x)\n",
    "\n",
    "# Plot the function\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=x, y=fx, color='red', label='f(x)', ax=ax)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('$f(x) = \\\\sin(2 \\\\pi x) + \\\\sin(4 \\\\pi x) + \\\\sin(7 \\\\pi x) + x$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a1a62",
   "metadata": {},
   "source": [
    "Constructing a training set by adding noise to a set of training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518cec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training samples\n",
    "n = 35\n",
    "# Generate n random x values in [0,1)\n",
    "x_train = np.linspace(start=0, stop=1, num=n) # np.random.uniform(0, 1, size=n)\n",
    "# Standard deviation of errors\n",
    "sigma_n = 0.4\n",
    "# Generate normally-distributed random observation errors\n",
    "epsilon = np.random.normal(loc=0, scale=sigma_n, size=n)\n",
    "# Add noise to observations\n",
    "y = f(x_train) + epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fa668",
   "metadata": {},
   "source": [
    " Plotting the distribution of errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bad385",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.histplot(epsilon, kde=True, ax=ax)\n",
    "ax.set(title='Error Distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a896780",
   "metadata": {},
   "source": [
    "Visualizing the noisy training data and the true function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3791a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=y, label='training data', ax=ax);\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=x, y=fx, color='red', label='f(x)', ax=ax);\n",
    "\n",
    "ax.set(title='Sample Data')\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1b209",
   "metadata": {},
   "source": [
    "Generating a set of test data points, $x_*$, for which we want to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_star = 100\n",
    "x_star = np.linspace(start=0, stop=1, num=n_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75109cbe",
   "metadata": {},
   "source": [
    "## Kernel Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d623c4",
   "metadata": {},
   "source": [
    "The covariance or kernel function $k(\\boldsymbol{x},\\boldsymbol{x'})$, which measures the degree of similarity between pairs of points, is often considered to be a function of the distance between $\\boldsymbol{x}$ and $\\boldsymbol{x'}$.\n",
    "\n",
    "We are flexible in specifying the mathematical form of the covariance (kernel) function, as long as the covariance matrix is positive semidefinite. Selecting the form of the covariance function also permits us to encode assumptions about our data. \n",
    "\n",
    "A common choice of kernel function to compute covariances is the squared exponential: \n",
    "\n",
    "$ \\text{Cov}\\left( f(x),f(x') \\right) = k(x,x') = \\sigma_f^2 \\exp \\left(-\\frac{1}{2\\ell^2} \\left( x - x' \\right) ^2 \\right) $, \n",
    "\n",
    "where $\\ell$ (lenght scale) and $\\sigma_f$ (signal variance) are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define squared exponential kernel function\n",
    "def kernel_function(x1, x2, length_scale=1.0, signal_variance=1.0):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential (RBF) kernel between two points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x1 : float\n",
    "        First input point.\n",
    "    x2 : float\n",
    "        Second input point.\n",
    "    length_scale : float, default=1.0\n",
    "        Length scale of the kernel (controls smoothness).\n",
    "    signal_variance : float, default=1.0\n",
    "        Signal variance (controls amplitude).\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kernel : float\n",
    "        Kernel value between x1 and x2.\n",
    "    \"\"\"    \n",
    "    # The kernel formula\n",
    "    kernel = signal_variance**2 * np.exp(-0.5 * (x1 - x2)**2 / length_scale**2)\n",
    "    \n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fe460c",
   "metadata": {},
   "source": [
    "Set hyperparameters of the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 0.1\n",
    "signal_variance = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4791aa",
   "metadata": {},
   "source": [
    "## Covariance Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425a175",
   "metadata": {},
   "source": [
    "A covariance matrix is constructed by evaluating the kernel function on all pairs of input data points. The kernel receives two points as an input and returns a similarity measure between those points in the form of a scalar.\n",
    "\n",
    "Let us denote covariance matrices appllied to train, $x_{train}$, and test, $x_*$, points by \n",
    "\n",
    "$ K(X,X) \\in M_{n \\times n}(\\mathbb{R}) $, \n",
    "$ K(X_*,X) \\in M_{n_* \\times n}(\\mathbb{R}) $, \n",
    "$ K(X_*,X_*) \\in M_{n_* \\times n_*}(\\mathbb{R}) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov_matrix(a, b, length_scale=1.0, signal_variance=1.0):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix between two sets of points using a kernel function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    a : np.ndarray, shape (n_a,)\n",
    "        First input array.\n",
    "    b : np.ndarray, shape (n_b,)\n",
    "        Second input array.\n",
    "    length_scale : float, default=1.0\n",
    "        Length scale for the kernel function.\n",
    "    signal_variance : float, default=1.0\n",
    "        Signal variance for the kernel function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    K : np.ndarray, shape (n_a, n_b)\n",
    "        Covariance matrix where each entry (i, j) is the result of \n",
    "        the kernel function applied to a[i] and b[j].\n",
    "    \"\"\"\n",
    "    n_a = a.shape[0]\n",
    "    n_b = b.shape[0]\n",
    "\n",
    "    K = [kernel_function(i, j, length_scale, signal_variance) for (i, j) in itertools.product(a, b)]\n",
    "    K = np.array(K).reshape(n_a, n_b)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5be10b",
   "metadata": {},
   "source": [
    "Computing the covariance matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d0fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K(X,X)\n",
    "K_xx = compute_cov_matrix(x_train, x_train, length_scale, signal_variance)\n",
    "# K(X*,X)\n",
    "K_star_x = compute_cov_matrix(x_star, x_train, length_scale, signal_variance)\n",
    "# K(X*,X*)\n",
    "K_star_star = compute_cov_matrix(x_star, x_star, length_scale, signal_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9492007f",
   "metadata": {},
   "source": [
    "## Joint Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ec160c",
   "metadata": {},
   "source": [
    "The joint distribution of training labels $y$ and $f_*$ (the predicted function values at test points $x_*$) is given by\n",
    "\n",
    "$ \\begin{pmatrix} y \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N}(0, \\boldsymbol{K}) $,\n",
    "\n",
    "where\n",
    "\n",
    "$ \\boldsymbol{K} = \n",
    "\\begin{pmatrix} \n",
    "K(X, X) + \\sigma_n^2 I & K(X, X_*) \\\\ \n",
    "K(X_*, X) & K(X_*, X_*)\n",
    "\\end{pmatrix} $,\n",
    "\n",
    "the $\\sigma_n^2 I$ term is added to the upper left component to account for noise in the observed points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7005de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the noise term to K(X,X)\n",
    "K_xx += (sigma_n**2) * np.eye(n)\n",
    "\n",
    "# Constructing the covariance matrix K from its components\n",
    "K = np.block([\n",
    "    [K_xx,     K_star_x.T ],\n",
    "    [K_star_x, K_star_star]\n",
    "    ])\n",
    "\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45209b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the covariance matrix K is symmetric\n",
    "print(np.all(K.T == K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26212c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the covariance matrix K is positive semi-definite\n",
    "eigvals = np.linalg.eigvalsh(K)\n",
    "print(np.all(eigvals >= -1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c708cf2",
   "metadata": {},
   "source": [
    "We can also visualize the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa0705",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(data=K, cmap='Blues', ax=ax)\n",
    "ax.set(title='Components of the Covariance Matrix K');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5442b4",
   "metadata": {},
   "source": [
    "## Prior Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba1c73",
   "metadata": {},
   "source": [
    "From the *consistency requirement* of gaussian processes we know that the prior distribution for target functions\n",
    "$f_*$ is \n",
    "\n",
    "$ \\mathcal{N}(0, K(X_*, X_*)) $.\n",
    "\n",
    "The *consistency requirement* (also known as the marginalization property) is ensuring that the marginal distributions derived from the joint distribution over any set of points match the distributions defined directly on subsets of those points. \n",
    "\n",
    "This property is automatically satisfied if the chosen kernel function $k(\\boldsymbol{x},\\boldsymbol{x'})$ is positive semidefinite. A positive semidefinite kernel guarantees that any covariance matrix $\\boldsymbol{K}$ generated by evaluating the kernel on a finite set of points will be positive semidefinite, which, as we already mentioned, is a requirement for a valid covariance matrix of a multivariate Gaussian distribution.\n",
    "\n",
    "Let us visualize some sample functions from this prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507adf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Sample from prior distribution\n",
    "    f_star = np.random.multivariate_normal(mean=np.zeros(n_star), cov=K_star_star)\n",
    "    # Plot function\n",
    "    sns.lineplot(x=x_star, y=f_star, color='blue', alpha=0.2, ax=ax)\n",
    "    \n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=x, y=fx, color='red', label='Target function f(x)', ax=ax)\n",
    "\n",
    "ax.set(title='Samples from Prior Distribution')\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f7121",
   "metadata": {},
   "source": [
    "## Conditional Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546160a",
   "metadata": {},
   "source": [
    "To get the posterior (conditional) distribution over functions, we need to restrict this prior distribution to contain only those functions which agree with the observed data points, that is, we are interested in computing $f_* \\vert X,y,X_*$. \n",
    "\n",
    "$ f_* \\vert X,y,X_* = \\mathcal{N}(\\bar{f}_*, \\text{Cov}(f_*)) $.\n",
    "\n",
    "The mean and covariance of this conditional distribution can be computed analyticaly, where the mean of the distribution i.e., our prediction is given by \n",
    "\n",
    "$ \\bar{f}_* = K(X_*, X)\\left( K(X, X) + \\sigma_n^2I \\right)^{-1} y $\n",
    "\n",
    "and covariance i.e., prediction uncertainty is given by\n",
    "\n",
    "$ \\text{Cov}(f_*) = K(X_*, X_*) - K(X_*, X) \\left( K(X, X) + \\sigma_n^2I \\right)^{-1} K(X, X_*) $.\n",
    "\n",
    "\n",
    "**Note**: computing the inverse of $ \\left( K(X, X) + \\sigma_n^2I \\right) $ is computationally expensive for larger data sets. A better approach is to use the Cholesky decomposition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e823cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute parameters of the posterior distribution\n",
    "def compute_gpr_parameters(K_xx, K_star_x, K_star_star, y):\n",
    "    \"\"\"\n",
    "    Compute the posterior mean and covariance of Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    K_xx : np.ndarray, shape (n, n)\n",
    "        Covariance matrix of training inputs.\n",
    "    K_star_x : np.ndarray, shape (n_star, n)\n",
    "        Covariance matrix between test and training inputs.\n",
    "    K_star_star : np.ndarray, shape (n_star, n_star)\n",
    "        Covariance matrix of test inputs.\n",
    "    y : np.ndarray, shape (n,)\n",
    "        Training target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    f_bar_star : np.ndarray, shape (n_star,)\n",
    "        Posterior mean of the predictions at the test points x_star.\n",
    "    cov_f_star : np.ndarray, shape (n_star, n_star)\n",
    "        Posterior covariance of the predictions at the test points.\n",
    "    \"\"\"\n",
    "    n = K_xx.shape[0]\n",
    "    d = 1  # Output dimension (assumes scalar targets)\n",
    "\n",
    "    # Compute inverse of K_xx\n",
    "    K_inv = np.linalg.inv(K_xx)\n",
    "\n",
    "    # Posterior mean\n",
    "    f_bar_star = np.dot(K_star_x, np.dot(K_inv, y.reshape([n, d])))\n",
    "\n",
    "    # Posterior covariance\n",
    "    cov_f_star = K_star_star - np.dot(K_star_x, np.dot(K_inv, K_star_x.T))\n",
    "\n",
    "    return f_bar_star, cov_f_star\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean and covariance using the above function\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(K_xx, K_star_x, K_star_star, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9794d",
   "metadata": {},
   "source": [
    "Let us visualize the covariance components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4414a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(data=cov_f_star, cmap='Blues', ax=ax)\n",
    "ax.set_title('Components of Posterior Covariance Matrix cov($f_*$)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ab2c25",
   "metadata": {},
   "source": [
    "Sample functions from the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fbe67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Sample from posterior distribution\n",
    "    f_star = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star)\n",
    "    # Plot function\n",
    "    sns.lineplot(x=x_star, y=f_star, color=\"blue\", alpha=0.1, ax=ax);\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=x, y=fx, color='red', label = 'Target function f(x)', ax=ax)\n",
    "\n",
    "ax.set(title=f'Samples from Posterior Distribution ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f852189a",
   "metadata": {},
   "source": [
    "## Hyperparameters of the Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e081708",
   "metadata": {},
   "source": [
    "We now explore effect of the hyperparameters $\\left( \\ell , \\sigma_f \\right)$ of the kernel function using two examples.\n",
    "- $\\ell$ (*length scale*): determines the degree of locality, i.e. how far the points correlate.\n",
    "- $\\sigma_f$ (*signal variance*): describes the amplitude of the function.\n",
    "\n",
    "Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e48ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 0.3\n",
    "signal_variance = 2.0\n",
    "\n",
    "# Compute components of the covariance matrix\n",
    "K_xx = compute_cov_matrix(x_train, x_train, length_scale, signal_variance) + (sigma_n**2) * np.eye(n)\n",
    "K_star_x = compute_cov_matrix(x_star, x_train, length_scale, signal_variance)\n",
    "K_star_star = compute_cov_matrix(x_star, x_star, length_scale, signal_variance)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(K_xx, K_star_x, K_star_star, y)\n",
    "\n",
    "# Plot posterior covariance matrix components\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(data=cov_f_star, cmap='Blues', ax=ax)\n",
    "ax.set(title=f'Components of the Covariance Matrix cov($f_*$) ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0eb4cf",
   "metadata": {},
   "source": [
    "In this case the values of the posterior covariance matrix are less localized. This means that points far away from each other still have some correlation. \n",
    "\n",
    "Let us sample functions from this posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb695a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Sample from posterior distribution\n",
    "    f_star = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star)\n",
    "    # Plot function\n",
    "    sns.lineplot(x=x_star, y=f_star, color=\"blue\", alpha=0.1, ax=ax);\n",
    "    \n",
    "# Plot the \"true\" function\n",
    "sns.lineplot(x=x, y=fx, color='red', label = 'Target function f(x)', ax=ax)\n",
    "# Plot the training data\n",
    "sns.scatterplot(x=x_train, y=y, color='gray', label='training data', ax=ax);\n",
    "\n",
    "ax.set(title=f'Samples from Posterior Distribution ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f91f8",
   "metadata": {},
   "source": [
    "A larger length scale results in a more global fit, thereby missing the more immediate changes in our target function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c6d70",
   "metadata": {},
   "source": [
    "Example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941dbd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 0.01\n",
    "signal_variance = 2.0\n",
    "\n",
    "# Compute components of the covariance matrix\n",
    "K_xx = compute_cov_matrix(x_train, x_train, length_scale, signal_variance) + (sigma_n**2) * np.eye(n)\n",
    "K_star_x = compute_cov_matrix(x_star, x_train, length_scale, signal_variance)\n",
    "K_star_star = compute_cov_matrix(x_star, x_star, length_scale, signal_variance)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(K_xx, K_star_x, K_star_star, y)\n",
    "\n",
    "# Plot posterior covariance matrix components\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(data=cov_f_star, cmap='Blues', ax=ax)\n",
    "ax.set(title=f'Components of the Covariance Matrix cov($f_*$) ($\\\\sigma_f$ = {signal_variance} \\\n",
    "and $\\\\ell$ = {length_scale})');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edbdf6a",
   "metadata": {},
   "source": [
    "In contrast, we see that for smaller values of $\\ell$, the values of the posterior covariance matrix are localized along the diagonal. This means that points far away have no effect on each other, i.e. the fit becomes more local. \n",
    "\n",
    "Let us sample functions from this posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Sample from posterior distribution\n",
    "    f_star = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star)\n",
    "    # Plot function\n",
    "    sns.lineplot(x=x_star, y=f_star, color=\"blue\", alpha=0.1, ax=ax);\n",
    "    \n",
    "# Plot the \"true\" function\n",
    "sns.lineplot(x=x, y=fx, color='red', label = 'Target function f(x)', ax=ax)\n",
    "# Plot the training data\n",
    "sns.scatterplot(x=x_train, y=y, color='gray', label='training data', ax=ax);\n",
    "\n",
    "ax.set(title=f'Samples from Posterior Distribution ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b419f6",
   "metadata": {},
   "source": [
    "Therefore, the kernel hyperparameter $\\ell$ encodes the “complexity” and “locality” of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d56cacd",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbadc07",
   "metadata": {},
   "source": [
    "We extend the range of $x$ values, in our target and predicted functions, to include a region not covered by the training set and will also compute the confidence intervals of the GP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evenly-spaced x values for prediction in an extended interval\n",
    "X = np.linspace(start=0, stop=1.2, num=600)\n",
    "X_star = np.linspace(start=0, stop=1.2, num=n_star)\n",
    "\n",
    "# Reset kernel hyperparameters\n",
    "length_scale = 0.1\n",
    "signal_variance = 2.0\n",
    "\n",
    "# Compute components of the covariance matrix\n",
    "K_xx = compute_cov_matrix(x_train, x_train, length_scale, signal_variance) + (sigma_n**2) * np.eye(n)\n",
    "K_star_x = compute_cov_matrix(X_star, x_train, length_scale, signal_variance)\n",
    "K_star_star = compute_cov_matrix(X_star, X_star, length_scale, signal_variance)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(K_xx, K_star_x, K_star_star, y)\n",
    "\n",
    "# Generate samples from posterior distribution\n",
    "y_hat_samples = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star, size=100)\n",
    "\n",
    "# Compute mean of samples \n",
    "y_hat = np.apply_over_axes(func=np.mean, a=y_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Compute standard deviation of samples\n",
    "y_hat_sd = np.apply_over_axes(func=np.std, a=y_hat_samples, axes=0).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed65ebd",
   "metadata": {},
   "source": [
    "Plotting the true function, training points, (mean) predicted function, and the confidence interval corresponding to two standard deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be1345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=y, label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=f(X), color='red', label='Target function f(x)', ax=ax);\n",
    "\n",
    "# Plot prediction (i.e., posterior mean)\n",
    "sns.lineplot(x=X_star, y=f_bar_star.squeeze(), color='black', label='Prediction', ax=ax)\n",
    "\n",
    "# Plot confidence interval\n",
    "ax.fill_between(\n",
    "    x=X_star, \n",
    "    y1=(y_hat - 2*y_hat_sd), \n",
    "    y2=(y_hat + 2*y_hat_sd), \n",
    "    color='green', \n",
    "    alpha = 0.3, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Prediction & Confidence Interval ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36851e62",
   "metadata": {},
   "source": [
    "GP predictions in regions not covered by the training set deviate from the true function and exhibit significantly higher variance, resulting in broader confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd911a",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6025af1",
   "metadata": {},
   "source": [
    "To optimize hyperparameters, $\\ell$ and $\\sigma_f$, of the kernel function, we perform a maximum likelihood estimation (MLE) by minimizing the negative logarithm of the marginal likelihood w.r.t. the kernel hyperparameters using a method such as BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dceab6",
   "metadata": {},
   "source": [
    "### Marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b8703",
   "metadata": {},
   "source": [
    "The marginal likelihood represents the probability of the observed data $y$ given the inputs $X$ and hyperparameters $\\theta$.\n",
    "\n",
    "For a Gaussian Process with mean $0$ and covariance $K$, the marginal likelihood of the observed data $y$ at inputs $X$ and hyperparameters $\\theta$ is \n",
    "\n",
    "$ \\mathcal{p} ( y \\vert X, \\theta ) = \\mathcal{N} \\left( y \\vert 0, K(X, X) + \\sigma_n^2 I \\right) $,\n",
    "\n",
    "where $\\theta = \\left( \\ell, \\sigma_f^2 \\right)$ are kernel hyperparameters (noise variance $\\sigma_n$ is also typically included in $\\theta$ and optimized along with kernel parameters).\n",
    "\n",
    "From a computational point of view, it is more convenient to work with the logarithm of the marginal likelihood, which reads as\n",
    "\n",
    "$ \\log \\mathcal{p}( y \\vert X, \\theta ) = \n",
    "-\\frac{1}{2} \\left( n \\log 2 \\pi + \\log \\vert K_n \\vert + y^{\\top} K_n^{-1} y \\right) $,\n",
    "\n",
    "where \n",
    "$ K_n = K(X, X) + \\sigma_n^2 I $.\n",
    "\n",
    "The first term, $n \\log 2 \\pi$, is a normalizing constant, the second term, $\\log \\lvert K_n \\rvert$, is a complexity penalty that penalizes overly complex models (i.e., large determinant = more model flexibility), while the third term, $y^{\\top} K_n^{-1} y$, which depends on the training outputs, measures how well the model fits the data. \n",
    "\n",
    "To optimize kernel hyperparameters $\\theta$, we perform a maximum likelihood estimation, which means we maximize the log marginal likelihood as a function of $\\theta$. Computationally, it is more convenient to reframe this estimation as a minimization of the negative logarithm of the marginal likelihood, \n",
    "\n",
    "$ \\hat{\\theta} = \\arg_{\\theta} \\min \\left( - \\log \\mathcal{p} \\left( y \\vert X, \\theta \\right) \\right) $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1602b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "# Define the log marginal likelihood\n",
    "def log_marginal_likelihood(params, x, y, sigma_n):\n",
    "    \"\"\"\n",
    "    Compute the negative log marginal likelihood of a Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : tuple of floats\n",
    "        Hyperparameters of the kernel: (length_scale, signal_variance).\n",
    "    x : np.ndarray, shape (n,)\n",
    "        Training input points.\n",
    "    y : np.ndarray, shape (n,)\n",
    "        Training target values.\n",
    "    sigma_n : float\n",
    "        Standard deviation of observation noise.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    negative_log_likelihood : float\n",
    "        The negative log marginal likelihood of the training data under the current hyperparameters.\n",
    "    \"\"\"\n",
    "    length_scale, signal_variance = params\n",
    "\n",
    "    # Compute covariance matrix with added noise variance\n",
    "    K_n = compute_cov_matrix(x, x, length_scale, signal_variance) + sigma_n ** 2 * np.eye(n)\n",
    "\n",
    "    # Compute inverse of the covariance matrix\n",
    "    K_n_inv = np.linalg.inv(K_n)\n",
    "\n",
    "    # Compute log determinant of K_n\n",
    "    log_det_K_n = np.log(np.linalg.det(K_n))\n",
    "\n",
    "    # Compute the negative log marginal likelihood\n",
    "    negative_log_likelihood = 0.5 * (\n",
    "        n * np.log(2 * np.pi) + log_det_K_n + np.dot(np.dot(y.T, K_n_inv), y)\n",
    "    )\n",
    "\n",
    "    return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b57bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess for (length_scale, signal_variance)\n",
    "initial_params = (1.0, 1.0)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "result = opt.minimize(\n",
    "    log_marginal_likelihood, \n",
    "    initial_params, \n",
    "    args=(x_train, y, sigma_n), \n",
    "    bounds=[(0.01, 10.0), (0.01, 10.0)], \n",
    "    method=\"L-BFGS-B\"\n",
    "    )\n",
    "\n",
    "# Get optimization results\n",
    "print(f'Optimization {\"Successful\" if result.success else \"Failed\"}!')\n",
    "\n",
    "optimal_length_scale, optimal_signal_variance = result.x\n",
    "print(\"Optimal length_scale:\", optimal_length_scale)\n",
    "print(\"Optimal signal_variance:\", optimal_signal_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6b021",
   "metadata": {},
   "source": [
    "We now plot the predicted function and the confidence interval using the optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034f9c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set kernel hyperparameters to optimized values\n",
    "length_scale = np.round(optimal_length_scale, decimals=2)\n",
    "signal_variance = np.round(optimal_signal_variance, decimals=2)\n",
    "\n",
    "# Compute components of the covariance matrix\n",
    "K_xx = compute_cov_matrix(x_train, x_train, length_scale, signal_variance) + (sigma_n**2) * np.eye(n)\n",
    "K_star_x = compute_cov_matrix(X_star, x_train, length_scale, signal_variance)\n",
    "K_star_star = compute_cov_matrix(X_star, X_star, length_scale, signal_variance)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(K_xx, K_star_x, K_star_star, y)\n",
    "\n",
    "# Generate samples from posterior distribution\n",
    "y_hat_samples = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star, size=100)\n",
    "\n",
    "# Compute mean of samples \n",
    "y_hat = np.apply_over_axes(func=np.mean, a=y_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Compute standard deviation of samples\n",
    "y_hat_sd = np.apply_over_axes(func=np.std, a=y_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=y, label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=f(X), color='red', label='Target function f(x)', ax=ax);\n",
    "\n",
    "# Plot prediction (i.e., posterior mean)\n",
    "sns.lineplot(x=X_star, y=f_bar_star.squeeze(), color='black', label='Prediction', ax=ax)\n",
    "\n",
    "# Plot confidence interval\n",
    "ax.fill_between(\n",
    "    x=X_star, \n",
    "    y1=(y_hat - 2*y_hat_sd), \n",
    "    y2=(y_hat + 2*y_hat_sd), \n",
    "    color='green', \n",
    "    alpha = 0.3, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Prediction & Confidence Interval ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f644a72",
   "metadata": {},
   "source": [
    "## Including Gradient of the Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a3d05",
   "metadata": {},
   "source": [
    "Since differentiation is a linear operator, the derivative of a Gaussian process (GP) is another GP. Thus we can use GPs to make predictions about derivatives, and also to make inference based on derivative information. \n",
    "In general, we can make inference based on the joint Gaussian distribution of function values and partial derivatives.\n",
    "\n",
    "A fundamental property of GPs is that applying a linear operator to a GP results in another GP. \n",
    "Therefore, if $f(x)$ is a GP, its partial derivatives, $\\partial f/\\partial x_i$ (where $x_i$ is the $i$-th component of $x$), are also GPs.\n",
    "\n",
    "When we want to learn both the function $f$ and its gradient $\\nabla f$ (which consists of all partial derivatives $\\partial f/\\partial x_i$), we are essentially dealing with a vector-valued GP output at each point $x_i$: $\\left[ f(x_i), \\partial f(x_i)/\\partial x_i \\right]$.\n",
    "\n",
    "Let $\\mathrm{\\textbf{y}_{ext}}$ be an extended observation set that also includes a set of derivative observations, \n",
    "\n",
    "$ \\mathrm{\\textbf{y}}_\\mathrm{ext}=  \\left[y_1,\\cdots,y_n,\\frac{\\partial f(x_1)}{\\partial x_1},\\cdots,\\frac{\\partial f(x_n)}{\\partial x_n}\\right]^\\top $.\n",
    "\n",
    "We start by defining the derivative of our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define derivative of the function\n",
    "def df(x):\n",
    "    df = np.cos(2 * np.pi * x) * 2 * np.pi \\\n",
    "       + np.cos(4 * np.pi * x) * 4 * np.pi \\\n",
    "       + np.cos(7 * np.pi * x) * 7 * np.pi \\\n",
    "       + 1\n",
    "    return df\n",
    "\n",
    "# Evaluate the function derivative at x points\n",
    "dfx = df(x)\n",
    "\n",
    "# Plot the function and its derivative\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x=x, y=fx, color='red', label='f(x)', ax=ax)\n",
    "sns.lineplot(x=x, y=dfx, color='magenta', label='df(x)/dx', ax=ax)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_title('Target function and its derivative');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47014272",
   "metadata": {},
   "source": [
    "Next, we need to generate noisy data of function's derivative at points $x_{train}$. Observed function values and derivatives may often have different noise levels, which are incorporated by using differing $\\sigma$ hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c950a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of errors\n",
    "sigma_d = 4.0\n",
    "# Generate normally-distributed random observation errors\n",
    "epsilon = np.random.normal(loc=0, scale=sigma_d, size=n)\n",
    "# Compute noisy observations\n",
    "dy = df(x_train) + epsilon\n",
    "# Extend the training labels to include derivativ values\n",
    "y_ext = np.concatenate((y, dy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3a52a",
   "metadata": {},
   "source": [
    "To perform regression using the extended set of observations, we need to model the joint covariance between all components at different input points. \n",
    "\n",
    "We need to calculate not just $\\text{Cov}\\left( f(x), f(x') \\right)$, but also: \n",
    "\n",
    "- $\\text{Cov}\\left( f(x), \\partial f(x')/\\partial x'_j \\right)$: Covariance between the function value at $x$ and the $j$-th partial derivative at $x'$\n",
    "\n",
    "- $\\text{Cov}\\left( \\partial f(x)/\\partial x_i, f(x') \\right)$: Covariance between the $i$-th partial derivative at $x$ and the function value at $x'$\n",
    "\n",
    "- $\\text{Cov}\\left( \\partial f(x)/\\partial x_i, \\partial f(x')/\\partial x'_j \\right)$: Covariance between the $i$-th partial derivative at $x$ and the $j$-th partial derivative at $x'$\n",
    "\n",
    "Under mild regularity conditions (which most standard kernel functions satisfy), we can interchange the order of differentiation and the covariance operation. \n",
    "Thus, using the definition $\\text{Cov}\\left( f(x), f(x') \\right) = k(x, x')$, we get: \n",
    "\n",
    "- $\\text{Cov}\\left( f(x), \\partial f(x')/\\partial x'_j \\right) = \\partial k(x, x')/\\partial x'_j$ \n",
    "(The partial derivative of the kernel with respect to the $j$-th component of its second argument)\n",
    "\n",
    "- $\\text{Cov}\\left( \\partial f(x)/\\partial x_i, f(x') \\right) = \\partial k(x, x')/\\partial x_i$ \n",
    "(The partial derivative of the kernel with respect to the i-th component of its first argument)\n",
    "\n",
    "- $\\text{Cov}\\left( \\partial f(x)/\\partial x_i, \\partial f(x')/\\partial x'_j \\right) = \\partial^2 k(x, x')/\\left(\\partial x_i \\partial x'_j\\right)$ \n",
    "(The second-order mixed partial derivative of the kernel)\n",
    "\n",
    "Accordingly, the extended kernel matrix is defined as\n",
    "\n",
    "$ K_{ext}\\left(X,X'\\right) = \n",
    "\\begin{pmatrix} \n",
    "k(x, x') & \\frac{\\partial}{\\partial x'} k\\left(x,x'\\right) \\\\ \n",
    "\\frac{\\partial}{\\partial x} k\\left(x,x'\\right) & \\frac{\\partial^2}{\\partial x \\partial x'} k\\left(x,x'\\right)\n",
    "\\end{pmatrix} $, \n",
    "\n",
    "with $n$ datapoints in $d$ dimensions, the complete joint distribution of $f$ and its $d$ partial derivatives involves $n(d+1)$ quantities.\n",
    "\n",
    "### Derivatives of the squared exponential kernel\n",
    "\n",
    "The first derivative of the squared exponential kernel with respect to $x$ (i.e., the covariance between a derivative value at $x$ and a function value at $x'$) is\n",
    "\n",
    "$ \\frac{\\partial}{\\partial x} k\\left(x,x'\\right) = -\\ell^{-2} \\left(x-x'\\right) k\\left(x,x'\\right) $,\n",
    "\n",
    "while the first derivative with respect to $x'$ (i.e., the covariance between a function value at $x$ and a derivative value at $x'$) is \n",
    "\n",
    "$ \\frac{\\partial}{\\partial x'} k\\left(x,x'\\right) = -\\frac{\\partial}{\\partial x} k\\left(x,x'\\right) $.\n",
    "\n",
    "The second derivative of the squared exponential kernel with respect to both of its inputs (i.e., the covariance between a derivative value at $x$ and a derivative value at $x'$) is\n",
    "\n",
    "$ \\frac{\\partial^2}{\\partial x \\partial x'} k\\left(x,x'\\right) = \\left( \\ell^{-2} - \\ell^{-4} \\left(x-x'\\right)^2 \\right) k\\left(x,x'\\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8038b385",
   "metadata": {},
   "source": [
    "Let us define a function to compute the first and second derivative of the squared exponential kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb36cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_derivative(x1, x2, length_scale, signal_variance):\n",
    "    \"\"\"\n",
    "    Compute the first and second derivative of the squared exponential kernel with respect to x1.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x1 : float\n",
    "        First input point.\n",
    "    x2 : float\n",
    "        Second input point.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dk : float\n",
    "        First derivative of the kernel with respect to x1.\n",
    "    d2k : float\n",
    "        Second derivative of the kernel with respect to x1 and x2.\n",
    "    \"\"\"\n",
    "    k = kernel_function(x1, x2, length_scale, signal_variance)\n",
    "    diff = x1 - x2\n",
    "    # First derivative\n",
    "    dk = - (diff / length_scale**2) * k\n",
    "    # Second derivative\n",
    "    d2k = ( (1 / length_scale**2) - (diff**2 / length_scale**4) ) * k\n",
    "    \n",
    "    return dk, d2k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54da76",
   "metadata": {},
   "source": [
    "Next, we compute the components of extended kernel matrix $K_{ext}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4eec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ext_cov_matrix(a, b, length_scale, signal_variance):\n",
    "    \"\"\"\n",
    "    Compute the covariance matrix between two sets of points using a kernel function, and its derivatives.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    a : np.ndarray, shape (n_a,)\n",
    "        First input array.\n",
    "    b : np.ndarray, shape (n_b,)\n",
    "        Second input array.\n",
    "    length_scale : float, default=1.0\n",
    "        Length scale for the kernel function.\n",
    "    signal_variance : float, default=1.0\n",
    "        Signal variance for the kernel function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    K_ext : np.ndarray, shape (n_a*2, n_b*2)\n",
    "        Covariance matrix where each entry (i, j) is the result of \n",
    "        the kernel function and its derivative functions applied to a[i] and b[j].\n",
    "    \"\"\"\n",
    "    n_a = a.shape[0]\n",
    "    n_b = b.shape[0]\n",
    "    K, dK, d2K = [], [], []\n",
    "    \n",
    "    for (i, j) in itertools.product(a, b):\n",
    "        \n",
    "        kernel = kernel_function(i, j, length_scale, signal_variance)\n",
    "        first_derivative, second_derivative = kernel_derivative(i, j, length_scale, signal_variance)\n",
    "        \n",
    "        K.append(kernel)\n",
    "        dK.append(first_derivative)\n",
    "        d2K.append(second_derivative)\n",
    "    \n",
    "    # Function-function\n",
    "    K = np.array(K).reshape(n_a, n_b)\n",
    "    # Derivative-function\n",
    "    dK = np.array(dK).reshape(n_a, n_b)\n",
    "    # Derivative-derivative\n",
    "    d2K = np.array(d2K).reshape(n_a, n_b)\n",
    "    \n",
    "    # Assemble the extended K matrix\n",
    "    K_ext = np.block([\n",
    "        [K,  -dK],\n",
    "        [dK, d2K]\n",
    "        ])\n",
    "\n",
    "    return K_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f88bb0",
   "metadata": {},
   "source": [
    "Constructing the full covariance matrix $\\boldsymbol{K}$ from its components: \n",
    "\n",
    "$ \\boldsymbol{K} = \n",
    "\\begin{pmatrix} \n",
    "K_{ext}(X, X) + \\sigma^2 I & K_{ext}(X, X_*) \\\\ \n",
    "K_{ext}(X_*, X) & K_{ext}(X_*, X_*)\n",
    "\\end{pmatrix} $.\n",
    "\n",
    "**Note**: the signal noise for observed values ($\\sigma_n$) and derivatives ($\\sigma_d$) should be added to the diagonal components of the extended kernel matrix $K_{ext}(X,X)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cb37f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K_ext(X,X)\n",
    "K_xx_ext = compute_ext_cov_matrix(x_train, x_train, length_scale, signal_variance)\n",
    "# K_ext(X*,X)\n",
    "K_star_x_ext = compute_ext_cov_matrix(X_star, x_train, length_scale, signal_variance)\n",
    "# K_ext(X*,X*)\n",
    "K_star_star_ext = compute_ext_cov_matrix(X_star, X_star, length_scale, signal_variance)\n",
    "\n",
    "# Add the noise value 𝜎_𝑛 to the upper left block (nxn) of K_xx_ext\n",
    "K_xx_ext[:n, :n] += (sigma_n**2) * np.eye(n)\n",
    "# Add the noise value 𝜎_d to the lower right block (nxn) of K_xx_ext\n",
    "K_xx_ext[n:, n:] += (sigma_d**2) * np.eye(n)\n",
    "\n",
    "K = np.block([\n",
    "    [K_xx_ext,     K_star_x_ext.T ],\n",
    "    [K_star_x_ext, K_star_star_ext]\n",
    "    ])\n",
    "\n",
    "print(K.shape)\n",
    "\n",
    "# Check if the covariance matrix K is symmetric\n",
    "print(np.all(K.T == K))\n",
    "\n",
    "# Check if the covariance matrix K is positive semi-definite\n",
    "eigvals = np.linalg.eigvalsh(K)\n",
    "print(np.all(eigvals >= -1e-8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f405c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean and covariance\n",
    "mean, cov = compute_gpr_parameters(K_xx_ext, K_star_x_ext, K_star_star_ext, y_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfbd90",
   "metadata": {},
   "source": [
    "Sample functions from the posterior distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad000aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Function samples from posterior distribution\n",
    "    f_star = np.random.multivariate_normal(mean=mean[:n_star].squeeze(), cov=cov[:n_star, :n_star])\n",
    "    # Plot function\n",
    "    sns.lineplot(x=X_star, y=f_star, color=\"blue\", alpha=0.1, ax=ax);\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=y, color='gray', label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=f(X), color='red', label = 'f(x)', ax=ax)\n",
    "\n",
    "ax.set(title=f'Samples of function from posterior ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})')\n",
    "ax.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3884efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # Derivative samples from posterior distribution\n",
    "    df_star = np.random.multivariate_normal(mean=mean[n_star:].squeeze(), cov=cov[n_star:, n_star:])\n",
    "    # Plot function\n",
    "    sns.lineplot(x=X_star, y=df_star, color=\"green\", alpha=0.1, ax=ax);\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=dy, color='gray', label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=df(X), color='magenta', label = 'df(x)/dx', ax=ax)\n",
    "\n",
    "ax.set(title=f\"Samples of function's derivative from posterior ($\\\\sigma_f$ = {signal_variance} and $\\\\ell$ = {length_scale})\")\n",
    "ax.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95460765",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed14fd0f",
   "metadata": {},
   "source": [
    "Again, we perform hyperparameter optimization to find the best values for length scale ($\\ell$), signal variance ($\\sigma_f$), and also signal noise ($\\sigma_n$ and $\\sigma_d$) by maximizing the log marginal likelihood.\n",
    "\n",
    "This time, we use the Cholesky decomposition to avoid directly computing $K_n^{-1} y$. \n",
    "\n",
    "$ K_n = LL^{\\top} $ \n",
    "\n",
    "with $L$ being a lower-triangular matrix.\n",
    "\n",
    "We first solve $Lz = y$ (forward substitution), then solve $L^{\\top}\\alpha = z$ (backward substitution), with \n",
    "\n",
    "$ \\alpha = K_n^{-1} y $.\n",
    "\n",
    "The log determinant of $K_n$ can also be computed using $L$ as \n",
    "\n",
    "$ \\log \\lvert K_n \\rvert = 2 \\sum \\log L_{ii} $ \n",
    "\n",
    "with $L_{ii}$ being the diagonal elements of the Cholesky factor $L$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the log marginal likelihood\n",
    "def log_marginal_likelihood_cholesky(params, x, y):\n",
    "    \"\"\"\n",
    "    Compute the negative log marginal likelihood of Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : tuple of floats\n",
    "        Hyperparameters of the kernel: (length_scale, signal_variance, sigma_n, sigma_d).\n",
    "    x : np.ndarray, shape (n,)\n",
    "        Training input points.\n",
    "    y : np.ndarray, shape (2*n,)\n",
    "        Training target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    negative_log_likelihood : float\n",
    "        The negative log marginal likelihood of the training data under the current hyperparameters.\n",
    "    \"\"\"\n",
    "    length_scale, signal_variance, sigma_n, sigma_d = params\n",
    "\n",
    "    # Compute covariance matrix\n",
    "    K_n = compute_ext_cov_matrix(x, x, length_scale, signal_variance)\n",
    "    # Add the noise value 𝜎_𝑛 to the upper left block\n",
    "    K_n[:n, :n] += (sigma_n**2) * np.eye(n)\n",
    "    # Add the noise value 𝜎_d to the lower right block\n",
    "    K_n[n:, n:] += (sigma_d**2) * np.eye(n)\n",
    "    \n",
    "    try:\n",
    "        L = np.linalg.cholesky(K_n)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.inf\n",
    "\n",
    "    # Solve Kn^−1.y using the Cholesky decomposition\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "    \n",
    "    # Compute the log determinant of Kn using its Cholesky factor\n",
    "    log_det_K_n = 2 * np.sum(np.log(np.diag(L)))\n",
    "\n",
    "    negative_log_likelihood = 0.5 * ( np.dot(y.T, alpha) + log_det_K_n + len(y) * np.log(2 * np.pi) )\n",
    "\n",
    "    return negative_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fb467",
   "metadata": {},
   "source": [
    "We optimize the kernel hyperparameters along with the signal noise $\\sigma_n$ and $\\sigma_d$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b32b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess for (length_scale, signal_variance, sigma_n, sigma_d)\n",
    "initial_params = (1.0, 1.0, 0.1, 0.1)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "result = opt.minimize(\n",
    "    log_marginal_likelihood_cholesky,\n",
    "    initial_params,\n",
    "    args=(x_train, y_ext),\n",
    "    bounds=[(0.01, 10.0), (0.01, 10.0), (0.01, 5.0), (0.01, 10.0)], \n",
    "    method=\"L-BFGS-B\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ef8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimization results\n",
    "print(f'Optimization {\"Successful\" if result.success else \"Failed\"}!')\n",
    "\n",
    "opt_length_scale, opt_signal_variance, opt_sigma_n, opt_sigma_d = result.x\n",
    "print(\"Optimal length_scale:\", opt_length_scale)\n",
    "print(\"Optimal signal_variance:\", opt_signal_variance)\n",
    "print(\"Optimal sigma_n:\", opt_sigma_n)\n",
    "print(\"Optimal sigma_d:\", opt_sigma_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8dc79c",
   "metadata": {},
   "source": [
    "We now plot the predicted function, its derivative and the confidence intervals using the optimized hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f063786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set kernel hyperparameters to optimized values\n",
    "length_scale = np.round(opt_length_scale, decimals=2)\n",
    "signal_variance = np.round(opt_signal_variance, decimals=2)\n",
    "sigma_n = np.round(opt_sigma_n, decimals=2)\n",
    "sigma_d = np.round(opt_sigma_d, decimals=2)\n",
    "\n",
    "# Compute components of the covariance matrix\n",
    "K_xx_ext = compute_ext_cov_matrix(x_train, x_train, length_scale, signal_variance)\n",
    "K_star_x_ext = compute_ext_cov_matrix(X_star, x_train, length_scale, signal_variance)\n",
    "K_star_star_ext = compute_ext_cov_matrix(X_star, X_star, length_scale, signal_variance)\n",
    "\n",
    "# Add the noise value 𝜎_𝑛 to the upper left block (nxn) of K_xx_ext\n",
    "K_xx_ext[:n, :n] += (sigma_n**2) * np.eye(n)\n",
    "# Add the noise value 𝜎_d to the lower right block (nxn) of K_xx_ext\n",
    "K_xx_ext[n:, n:] += (sigma_d**2) * np.eye(n)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "mean, cov = compute_gpr_parameters(K_xx_ext, K_star_x_ext, K_star_star_ext, y_ext)\n",
    "\n",
    "# Generate samples from posterior distribution\n",
    "y_hat_samples = np.random.multivariate_normal(mean=mean[:n_star].squeeze(), cov=cov[:n_star, :n_star], size=100)\n",
    "\n",
    "# Compute mean of samples \n",
    "y_hat = np.apply_over_axes(func=np.mean, a=y_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Compute standard deviation of samples\n",
    "y_hat_sd = np.apply_over_axes(func=np.std, a=y_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=y, color='gray', label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=f(X), color='red', label = 'f(x)', ax=ax)\n",
    "\n",
    "# Plot prediction (i.e., posterior mean)\n",
    "sns.lineplot(x=X_star, y=mean[:n_star].squeeze(), color='black', label='Prediction', ax=ax)\n",
    "\n",
    "# Plot confidence interval\n",
    "ax.fill_between(\n",
    "    x=X_star, \n",
    "    y1=(y_hat - 2*y_hat_sd), \n",
    "    y2=(y_hat + 2*y_hat_sd), \n",
    "    color='blue', \n",
    "    alpha = 0.3, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Prediction & Confidence Interval ($\\\\sigma_f$ = {signal_variance}, \\\n",
    "$\\\\ell$ = {length_scale}, $\\\\sigma_n$ = {sigma_n}, $\\\\sigma_d$ = {sigma_d})')\n",
    "ax.legend(loc='lower left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222bcf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples from posterior distribution\n",
    "dy_hat_samples = np.random.multivariate_normal(mean=mean[n_star:].squeeze(), cov=cov[n_star:, n_star:], size=100)\n",
    "\n",
    "# Compute mean of samples \n",
    "dy_hat = np.apply_over_axes(func=np.mean, a=dy_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Compute standard deviation of samples\n",
    "dy_hat_sd = np.apply_over_axes(func=np.std, a=dy_hat_samples, axes=0).squeeze()\n",
    "\n",
    "# Plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot training data\n",
    "sns.scatterplot(x=x_train, y=dy, color='gray', label='Training data', ax=ax)\n",
    "\n",
    "# Plot \"true\" function\n",
    "sns.lineplot(x=X, y=df(X), color='magenta', label = 'df(x)/dx', ax=ax)\n",
    "\n",
    "# Plot prediction (i.e., posterior mean)\n",
    "sns.lineplot(x=X_star, y=mean[n_star:].squeeze(), color='black', label='Prediction', ax=ax)\n",
    "\n",
    "# Plot confidence interval\n",
    "ax.fill_between(\n",
    "    x=X_star, \n",
    "    y1=(dy_hat - 2*dy_hat_sd), \n",
    "    y2=(dy_hat + 2*dy_hat_sd), \n",
    "    color='green', \n",
    "    alpha = 0.3, \n",
    "    label='Confidence Interval'\n",
    ")\n",
    "\n",
    "ax.set_title(f'Prediction & Confidence Interval ($\\\\sigma_f$ = {signal_variance}, \\\n",
    "$\\\\ell$ = {length_scale}, $\\\\sigma_n$ = {sigma_n}, $\\\\sigma_d$ = {sigma_d})')\n",
    "ax.legend(loc='lower left');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
