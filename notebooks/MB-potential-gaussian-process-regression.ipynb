{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6940809b",
   "metadata": {},
   "source": [
    "# Modeling the Müller-Brown Potential using Gaussian process regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fec482",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "## Defining the Müller-Brown Potential\n",
    "\n",
    "The Müller-Brown potential is a well-known model potential energy surface used in computational chemistry to study reaction dynamics and transition states. It is defined by a sum of four Gaussian-like terms, and its mathematical form is (for more details see [here](https://www.wolframcloud.com/objects/demonstrations/TrajectoriesOnTheMullerBrownPotentialEnergySurface-source.nb)): \n",
    "\n",
    "$ V(x,y) = \\sum_{k=1}^4 A_k \\mathrm{exp}\\left( a_k \\left( x - x_k^0 \\right)^2 + b_k \\left( x - x_k^0 \\right) \\left( y - y_k^0 \\right) + c_k \\left( y - y_k^0 \\right)^2 \\right) $.\n",
    "\n",
    "First, we define the Müller-Brown potential as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f84de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mueller_brown_potential(x, y):\n",
    "    \"\"\"\n",
    "    Compute the Müller-Brown potential energy function at position (x, y).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : float\n",
    "        x-coordinate of the point.\n",
    "    y : float\n",
    "        y-coordinate of the point.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    value : float\n",
    "        Potential energy at the given (x, y) position.\n",
    "    \"\"\"\n",
    "    # Amplitude parameters\n",
    "    A = [-200, -100, -170, 15]\n",
    "    \n",
    "    # Shape coefficients\n",
    "    a = [-1, -1, -6.5, 0.7]\n",
    "    b = [0, 0, 11, 0.6]\n",
    "    c = [-10, -10, -6.5, 0.7]\n",
    "    \n",
    "    # Center coordinates of the minima\n",
    "    x_0 = [1.0, 0.0, -0.5, -1.0]\n",
    "    y_0 = [0.0, 0.5, 1.5, 1.0]\n",
    "\n",
    "    v_xy = 0.0\n",
    "\n",
    "    for k in range(4):\n",
    "        dx = x - x_0[k]\n",
    "        dy = y - y_0[k]\n",
    "        # Scale the function by 0.1 to make visualization easier\n",
    "        v_xy += 0.1 * A[k] * np.exp(a[k] * dx**2 + b[k] * dx * dy + c[k] * dy**2)\n",
    "\n",
    "    return v_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdece7",
   "metadata": {},
   "source": [
    "## Generate Training Data\n",
    "\n",
    "Next, we need to generate data points to train a Gaussian process regression model. The training data will be generated using the Müller-Brown Potential function and a range of x and y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8112414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of x and y values\n",
    "x = np.arange(-1.8, 1.4, 0.05)\n",
    "y = np.arange(-0.8, 2.4, 0.05)\n",
    "\n",
    "# Pair x and y values\n",
    "xy = [(i,j) for j in y for i in x]\n",
    "\n",
    "# Calculate the function value for all pairs\n",
    "v = [mueller_brown_potential(i,j) for j in y for i in x]\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "xy, v = np.array(xy), np.array(v)\n",
    "\n",
    "# Create a rectangular grid from given one-dimensional arrays x and y\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Reshape v array so that we can plot our data on a 2D surface that is len(x) by len(y)\n",
    "V = np.reshape(v, (len(x), -1))\n",
    "\n",
    "# Print some statistics\n",
    "print(f'V_min: {np.amin(V)},  V_max: {np.amax(V)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5cdae",
   "metadata": {},
   "source": [
    "## Visualizing MB Potential: 3D Surface\n",
    "\n",
    "We will now create a 3D plot of our data. To make the plot more readable, we will exclude the points that have extremely high energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a31e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Surface(z=V, x=X, y=Y, colorscale='rainbow', cmin=-15, cmax=9)])\n",
    "\n",
    "# Add contour lines\n",
    "fig.update_traces(contours_z=dict(show=True, project_z=True, width=1))\n",
    "\n",
    "# Format the layout\n",
    "fig.update_layout(\n",
    "    title='Müller-Brown PES', \n",
    "    width=600, height=600,\n",
    "    scene = dict(\n",
    "        zaxis_title=\"V(x,y)\",\n",
    "        zaxis = dict(dtick=3, range=[-15, 15]),\n",
    "        camera_eye = dict(x=-1.2, y=-1.2, z=1.0)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd64de6e",
   "metadata": {},
   "source": [
    "## Visualizing MB Potential: Contour Plot\n",
    "\n",
    "To allow for an easier visualization of the potential energy surface, we can generate a 2D contour surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e267fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Define contour levels\n",
    "levels = np.arange(-12, 8, 2)\n",
    "\n",
    "# Contour lines\n",
    "contour_lines = plt.contour(X, Y, V, levels=levels, colors='k', linewidths=1.0)\n",
    "plt.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=8)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = plt.contourf(X, Y, V, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "plt.xlabel(\"x\", labelpad=2.5)\n",
    "plt.ylabel(\"y\", labelpad=2.5)\n",
    "plt.tick_params(axis='both', pad=2, labelsize=8)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(contour_fill)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "# Title\n",
    "plt.title('Müller-Brown Contour Surface', fontsize=10)\n",
    "\n",
    "# Layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae53d402",
   "metadata": {},
   "source": [
    "Generating some noisy training sample observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a7188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select every 2nd point for training\n",
    "xy_train, v_train = xy[::2].copy(), v[::2].copy()\n",
    "# Standard deviation of errors\n",
    "sigma_n = 0.2\n",
    "# Generate normally-distributed random observation errors\n",
    "epsilon = np.random.normal(loc=0, scale=sigma_n, size=len(v_train))\n",
    "# Compute noisy observations\n",
    "v_train += epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0602dad",
   "metadata": {},
   "source": [
    "Generate test data points on which we want to generate predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a5e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of x and y values\n",
    "x_star = np.linspace(start=-1.8, stop=1.4, num=20)\n",
    "y_star = np.linspace(start=-0.8, stop=2.4, num=20)\n",
    "\n",
    "# Pair x and y values\n",
    "xy_star = [(i,j) for j in y_star for i in x_star]\n",
    "\n",
    "# Convert list to numpy array\n",
    "xy_star = np.array(xy_star)\n",
    "\n",
    "# Create a rectangular grid for plotting\n",
    "X_star, Y_star = np.meshgrid(x_star, y_star)\n",
    "\n",
    "n_star = len(xy_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cefe9",
   "metadata": {},
   "source": [
    "## Kernel Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1236795",
   "metadata": {},
   "source": [
    "A common choice of kernel function to compute covariances is the squared exponential (RBF): \n",
    "\n",
    "$ \\text{Cov}\\left( f(x),f(x') \\right) = k(x,x') = \\sigma_f^2 \\exp \\left(-\\frac{1}{2\\ell^2} \\left( x - x' \\right) ^2 \\right) $, \n",
    "\n",
    "where $\\ell$ (lenght scale) and $\\sigma_f$ (signal variance) are hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a05da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define squared exponential kernel function\n",
    "def kernel_function(X1, X2, length_scale=1.0, signal_variance=1.0):\n",
    "    \"\"\"\n",
    "    Computes the squared exponential (RBF) kernel between two sets of points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X1 : np.ndarray, shape (n_1, 2)\n",
    "        First set of input points.\n",
    "    X2 : np.ndarray, shape (n_2, 2)\n",
    "        Second set of input points.\n",
    "    length_scale : float, default=1.0\n",
    "        Length scale of the kernel (controls smoothness).\n",
    "    signal_variance : float, default=1.0\n",
    "        Signal variance (controls amplitude).\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    kernel : np.ndarray, shape (n_1, n_2)\n",
    "        Covariance matrix between X1 and X2.\n",
    "    \"\"\"    \n",
    "    # The kernel formula\n",
    "    squared_distance = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n",
    "    kernel = signal_variance**2 * np.exp(-0.5 * squared_distance / length_scale**2)\n",
    "    \n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62184048",
   "metadata": {},
   "source": [
    "Set hyperparameters of the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bab448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scale = 0.1\n",
    "signal_variance = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cd804",
   "metadata": {},
   "source": [
    "## Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov_matrix(xy_train, xy_test, length_scale, signal_variance, sigma_n):\n",
    "    \"\"\"\n",
    "    Computing components of the covariance matrix of the joint distribution of train and test points, \n",
    "    where K = K(X,X), K_star = K(X*,X), and K_star_star = K(X*,X*).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    xy_train : np.ndarray, shape (n_train, 2)\n",
    "        First input array.\n",
    "    xy_test : np.ndarray, shape (n_test, 2)\n",
    "        Second input array.\n",
    "    length_scale : float\n",
    "        Length scale for the kernel function.\n",
    "    signal_variance : float\n",
    "        Signal variance for the kernel function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    C : np.ndarray, shape (n_train + n_test, n_train + n_test)\n",
    "        Full covariance matrix C and its components K, K*, K**\n",
    "    \"\"\"\n",
    "    n_train = len(xy_train)\n",
    "    \n",
    "    # K(X,X)\n",
    "    K = kernel_function(xy_train, xy_train, length_scale, signal_variance)\n",
    "    \n",
    "    # Add the noise value 𝜎_𝑛 to K(X,X)\n",
    "    K += sigma_n**2 * np.eye(n_train)\n",
    "    \n",
    "    # K(X*,X)\n",
    "    K_star = kernel_function(xy_test, xy_train, length_scale, signal_variance)\n",
    "    \n",
    "    # K(X*,X*)\n",
    "    K_star_star = kernel_function(xy_test, xy_test, length_scale, signal_variance)\n",
    "    \n",
    "    # Constructing the full covariance matrix from its components\n",
    "    C = np.block([\n",
    "        [K,      K_star.T   ],\n",
    "        [K_star, K_star_star]\n",
    "        ])\n",
    "    \n",
    "    return K, K_star, K_star_star, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full covariance matrix\n",
    "C = compute_cov_matrix(xy_train, xy_star, length_scale, signal_variance, sigma_n)\n",
    "\n",
    "C[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the covariance matrix C is symmetric\n",
    "def is_symmetric(A, tol=1e-8):\n",
    "    return np.allclose(A, A.T, atol=tol)\n",
    "\n",
    "# Check if the covariance matrix C is positive semi-definite\n",
    "def is_positive_semi_definite(A, tol=1e-8):\n",
    "    eigvals = np.linalg.eigvalsh(A)\n",
    "    return np.all(eigvals >= -tol)\n",
    "\n",
    "print(is_symmetric(C[3]))\n",
    "print(is_positive_semi_definite(C[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ece695b",
   "metadata": {},
   "source": [
    "## Prior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ef745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x3 subplot grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6.5))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Sample from prior\n",
    "    f_star = np.random.multivariate_normal(mean=np.zeros(len(C[2])), cov=C[2])\n",
    "    \n",
    "    # Reshape f_star so that we can plot our data on a 2D surface\n",
    "    F_star = np.reshape(f_star, (len(x_star), -1))\n",
    "    \n",
    "    # Plot contour lines\n",
    "    contour_lines = ax.contour(X_star, Y_star, F_star, levels=levels, colors='k', linewidths=0.8)\n",
    "    ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "    \n",
    "    # Filled contours\n",
    "    contour_fill = ax.contourf(X_star, Y_star, F_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "    \n",
    "    # Axis labels and ticks\n",
    "    ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "    ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "    ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "    ax.set_title(f'Sample {i+1}', fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Samples from Prior Distribution\", fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdaaf5b1",
   "metadata": {},
   "source": [
    "## Conditional (Posterior) Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute parameters of the posterior distribution\n",
    "def compute_gpr_parameters(C, y):\n",
    "    \"\"\"\n",
    "    Compute the posterior mean and covariance of Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    C : np.ndarray, shape (n_train + n_test, n_train + n_test)\n",
    "        Full covariance matrix.\n",
    "    y : np.ndarray, shape (n_train,)\n",
    "        Training target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    f_bar_star : np.ndarray, shape (n_star,)\n",
    "        Posterior mean of the predictions at the test inputs.\n",
    "    cov_f_star : np.ndarray, shape (n_star, n_star)\n",
    "        Posterior covariance of the predictions at the test inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    L = np.linalg.cholesky(C[0])\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "\n",
    "    # Posterior mean\n",
    "    f_bar_star = C[1] @ alpha\n",
    "    \n",
    "    v = np.linalg.solve(L, C[1].T)\n",
    "\n",
    "    # Posterior covariance\n",
    "    cov_f_star = C[2] - v.T @ v\n",
    "\n",
    "    return f_bar_star, cov_f_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean and covariance using the above function\n",
    "f_bar_star, cov_f_star = compute_gpr_parameters(C, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x3 subplot grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6.5))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Sample from posterior\n",
    "    f_star = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star)\n",
    "    \n",
    "    # Reshape f_star so that we can plot our data on a 2D surface\n",
    "    F_star = np.reshape(f_star, (len(x_star), -1))\n",
    "    \n",
    "    # Plot contour lines\n",
    "    contour_lines = ax.contour(X_star, Y_star, F_star, levels=levels, colors='k', linewidths=0.8)\n",
    "    ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "    \n",
    "    # Filled contours\n",
    "    contour_fill = ax.contourf(X_star, Y_star, F_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "    \n",
    "    # Axis labels and ticks\n",
    "    ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "    ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "    ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "    ax.set_title(f'Sample {i+1}', fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Samples from Posterior Distribution\", fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261f975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1x2 subplot grid\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 5))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "# Plot the predicted surface as an average of n_samples from posterior\n",
    "ax = axes[0]\n",
    "\n",
    "\n",
    "# Samples from posterior\n",
    "f_star = np.random.multivariate_normal(mean=f_bar_star.squeeze(), cov=cov_f_star, size=20)\n",
    "\n",
    "# Mean of samples\n",
    "f_star = np.mean(f_star, axis=0)\n",
    "\n",
    "# Reshape f_star so that we can plot our data on a 2D surface\n",
    "F_star = np.reshape(f_star, (len(x_star), -1))\n",
    "\n",
    "# Contour lines\n",
    "contour_lines = ax.contour(X_star, Y_star, F_star, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X_star, Y_star, F_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Predicted Surface', fontsize=8)\n",
    "\n",
    "# Plot the true potential energy surface\n",
    "ax = axes[1]\n",
    "\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, V, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, V, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'True Surface', fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Predicted vs True Function\", fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324d2b3",
   "metadata": {},
   "source": [
    "## Optimizing the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afb76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Log Marginal Likelihood\n",
    "def log_marginal_likelihood(params, train_points, train_labels):\n",
    "    \"\"\"\n",
    "    Compute the negative log marginal likelihood of Gaussian Process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    params : tuple of floats\n",
    "        Hyperparameters: length_scale, signal_variance, sigma_n.\n",
    "    train_points : np.ndarray, shape (n_train, 2)\n",
    "        Training input coordinates.\n",
    "    train_labels : np.ndarray, shape (n_train,)\n",
    "        Training target values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    negative_log_likelihood : float\n",
    "        The negative log marginal likelihood of the training data under the current hyperparameters.\n",
    "    \"\"\"\n",
    "    length_scale, signal_variance, sigma_n = params\n",
    "    \n",
    "    # Compute covariance matrix K\n",
    "    K = kernel_function(train_points, train_points, length_scale, signal_variance)\n",
    "    K += sigma_n**2 * np.eye(len(train_points))\n",
    "\n",
    "    try:\n",
    "        L = np.linalg.cholesky(K)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return np.inf\n",
    "\n",
    "    # Solve K^−1.y using the Cholesky decomposition\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, train_labels))\n",
    "    \n",
    "    # Compute the log determinant of K using its Cholesky factor\n",
    "    log_det_K = 2 * np.sum(np.log(np.diag(L)))\n",
    "    \n",
    "    # Compute negative log marginal likelihood \n",
    "    NLL = 0.5 * ( train_labels.T @ alpha + log_det_K + len(train_labels) * np.log(2 * np.pi) )\n",
    "    \n",
    "    return NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial guess for (length_scale, signal_variance, sigma_n)\n",
    "initial_params = (1.0, 1.0, 0.5)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "result = opt.minimize(\n",
    "    log_marginal_likelihood,\n",
    "    initial_params,\n",
    "    args=(xy_train, v_train),\n",
    "    bounds=[(0.01, 10.0), (0.01, 10.0), (0.01, 5.0)], \n",
    "    method=\"L-BFGS-B\"\n",
    "    )\n",
    "\n",
    "# Get optimization results\n",
    "print(f'Optimization {\"Successful\" if result.success else \"Failed\"}!')\n",
    "\n",
    "opt_length_scale, opt_signal_variance, opt_sigma_n = result.x\n",
    "print(\"Optimal length_scale:\", opt_length_scale)\n",
    "print(\"Optimal signal_variance:\", opt_signal_variance)\n",
    "print(\"Optimal sigma_n:\", opt_sigma_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f8c3f2",
   "metadata": {},
   "source": [
    "## Building a GPR Model using Energies and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e93928",
   "metadata": {},
   "source": [
    "We start by defining derivatives of the Müller-Brown function with respect to $x$ and $y$:\n",
    "\n",
    "$ \\frac{\\partial V}{\\partial x}  = \\sum_{k=1}^4 A_k \\mathrm{exp}\\left( a_k \\left(x - x_k^0\\right)^2 + b_k \\left(x - x_k^0\\right) \\left(y - y_k^0\\right) + c_k \\left(y - y_k^0\\right)^2 \\right) \\left( 2a_k \\left(x - x_k^0\\right) + b_k \\left(y - y_k^0\\right) \\right) $,\n",
    "\n",
    "$ \\frac{\\partial V}{\\partial y}  = \\sum_{k=1}^4 A_k \\mathrm{exp}\\left( a_k \\left(x - x_k^0\\right)^2 + b_k \\left(x - x_k^0\\right) \\left(y - y_k^0\\right) + c_k \\left(y - y_k^0\\right)^2 \\right) \\left( b_k \\left(x - x_k^0\\right) + 2 c_k \\left(y - y_k^0\\right) \\right) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7359799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mueller_brown_potential_gradient(x, y):\n",
    "    \"\"\"\n",
    "    Compute the partial derivatives of Müller-Brown potential energy function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : float\n",
    "        x-coordinate of the point.\n",
    "    y : float\n",
    "        y-coordinate of the point.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    value : float\n",
    "        Partial derivatives of potential energy at the given (x, y) position.\n",
    "    \"\"\"\n",
    "    # Amplitude parameters\n",
    "    A = [-200, -100, -170, 15]\n",
    "    \n",
    "    # Shape coefficients\n",
    "    a = [-1, -1, -6.5, 0.7]\n",
    "    b = [0, 0, 11, 0.6]\n",
    "    c = [-10, -10, -6.5, 0.7]\n",
    "    \n",
    "    # Center coordinates of the minima\n",
    "    x_0 = [1.0, 0.0, -0.5, -1.0]\n",
    "    y_0 = [0.0, 0.5, 1.5, 1.0]\n",
    "\n",
    "    dv_x, dv_y = 0.0, 0.0\n",
    "\n",
    "    for k in range(4):\n",
    "        dx = x - x_0[k]\n",
    "        dy = y - y_0[k]\n",
    "        # Scale the function by 0.1 to make visualization easier\n",
    "        dv_x += 0.1 * A[k] * np.exp(a[k] * dx**2 + b[k] * dx * dy + c[k] * dy**2) * (2 * a[k] * dx + b[k] * dy)\n",
    "        dv_y += 0.1 * A[k] * np.exp(a[k] * dx**2 + b[k] * dx * dy + c[k] * dy**2) * (b[k] * dx + 2 * c[k] * dy)\n",
    "\n",
    "    return dv_x, dv_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bf18d3",
   "metadata": {},
   "source": [
    "Let $\\mathrm{\\textbf{y}_{ext}}$ be an extended observation set that also includes a set of derivative observations, \n",
    "\n",
    "$ \\mathrm{\\textbf{y}}_\\mathrm{ext}=  \\left[y_1,\\cdots,y_n,\\frac{\\partial f(\\textbf{x}_1)}{\\partial x},\\cdots,\\frac{\\partial f(\\textbf{x}_n)}{\\partial x},\\frac{\\partial f(\\textbf{x}_1)}{\\partial y},\\cdots,\\frac{\\partial f(\\textbf{x}_n)}{\\partial y}\\right]^{\\top} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the derivative value for all pairs\n",
    "dv = [mueller_brown_potential_gradient(i,j) for j in y for i in x]\n",
    "# Convert list to numpy array\n",
    "dv = np.array(dv)\n",
    "# Slice and reshape dv array so that we can plot derivatives on 2D surfaces\n",
    "dV_x = np.reshape(dv[:, 0], (len(x), -1))\n",
    "dV_y = np.reshape(dv[:, 1], (len(y), -1))\n",
    "\n",
    "# Select every 2nd point for training\n",
    "dv_train = dv[::2].copy()\n",
    "# Standard deviation of errors\n",
    "sigma_d = 0.1\n",
    "# Generate normally-distributed random observation errors\n",
    "epsilon = np.random.normal(loc=0, scale=sigma_d, size=dv_train.shape)\n",
    "# Compute noisy derivative observations\n",
    "dv_train += epsilon\n",
    "\n",
    "dv_x = dv_train[:, 0]\n",
    "dv_y = dv_train[:, 1]\n",
    "y_ext = np.concatenate((v_train, dv_x, dv_y))\n",
    "\n",
    "y_ext.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a483e",
   "metadata": {},
   "source": [
    "Let us visualize the Müller-Brown potential energy surface along with its gradient surfaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dbec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1x3 subplot grid\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3.2))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "# Plot the potential energy surface\n",
    "ax = axes[0]\n",
    "\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, V, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, V, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Energy Surface V(x,y)', fontsize=8)\n",
    "\n",
    "# Plot the dx surface\n",
    "ax = axes[1]\n",
    "\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, dV_x, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, dV_x, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Gradient Surface dV/dx', fontsize=8)\n",
    "\n",
    "# Plot the dy surface\n",
    "ax = axes[2]\n",
    "\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, dV_y, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, dV_y, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Gradient Surface dV/dy', fontsize=8)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_derivative(X1, X2, length_scale=1.0, signal_variance=1.0):\n",
    "    \"\"\"\n",
    "    Computes the kernel matrix including function values and partial derivatives.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X1 : np.ndarray, shape (n_1, 2)\n",
    "        First input array.\n",
    "    X2 : np.ndarray, shape (n_2, 2)\n",
    "        Second input array.\n",
    "    length_scale : float, default=1.0\n",
    "        Length scale for the kernel function.\n",
    "    signal_variance : float, default=1.0\n",
    "        Signal variance for the kernel function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    K_full : np.ndarray, shape (3*n_1, 3*n_2)\n",
    "        Combined kernel block matrix with values and derivatives.\n",
    "    \"\"\"\n",
    "    # k(x,x')\n",
    "    K = kernel_function(X1, X2, length_scale, signal_variance)\n",
    "\n",
    "    # (x_i - x'_j) / l²\n",
    "    dx = np.subtract.outer(X1[:, 0], X2[:, 0]) / length_scale**2\n",
    "    # (y_i - y'_j) / l²\n",
    "    dy = np.subtract.outer(X1[:, 1], X2[:, 1]) / length_scale**2\n",
    "    \n",
    "    # Cov(f,f') = k(x,x')\n",
    "    K_f_f = K\n",
    "\n",
    "    # Cov(∂f/∂x,f') = ∂k(x,x')/∂x\n",
    "    K_dx_f = - dx * K\n",
    "    \n",
    "    # Cov(f,∂f'/∂x') = ∂k(x,x')/∂x' = - Cov(∂f/∂x,f')\n",
    "    K_f_dx = - K_dx_f\n",
    "    \n",
    "    # Cov(∂f/∂y,f') = ∂k(x,x')/∂y\n",
    "    K_dy_f = - dy * K\n",
    "    \n",
    "    # Cov(f,∂f'/∂y') = ∂k(x,x')/∂y' = - Cov(∂f/∂y,f')\n",
    "    K_f_dy = - K_dy_f\n",
    "\n",
    "    # Cov(∂f/∂x,∂f'/∂x') = ∂²k/∂x∂x'\n",
    "    K_dx_dx = (1 / length_scale**2 - dx**2) * K\n",
    "    \n",
    "    # Cov(∂f/∂y,∂f'/∂y') = ∂²k/∂y∂y'\n",
    "    K_dy_dy = (1 / length_scale**2 - dy**2) * K\n",
    "    \n",
    "    # Cov(∂f/∂y,∂f'/∂x') = ∂²k/∂y∂x'\n",
    "    K_dy_dx = - dx * dy * K\n",
    "    \n",
    "    # Cov(∂f/∂x,∂f'/∂y') = ∂²k/∂x∂y' = Cov(∂f/∂y,∂f'/∂x')\n",
    "    K_dx_dy = K_dy_dx\n",
    "\n",
    "    # Assemble full matrix\n",
    "    K_full = np.block([\n",
    "        [ K_f_f,   K_f_dx,   K_f_dy  ],\n",
    "        [ K_dx_f,  K_dx_dx,  K_dx_dy ],\n",
    "        [ K_dy_f,  K_dy_dx,  K_dy_dy ]\n",
    "    ])\n",
    "\n",
    "    return K_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ext_cov_matrix(xy_train, xy_test, length_scale, signal_variance, sigma_n, sigma_d):\n",
    "    \"\"\"\n",
    "    Computing components of the covariance matrix of the joint distribution of train and test points, \n",
    "    where K = K(X,X), K_star = K(X*,X), and K_star_star = K(X*,X*).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    xy_train : np.ndarray, shape (n_train, 2)\n",
    "        First input array.\n",
    "    xy_test : np.ndarray, shape (n_test, 2)\n",
    "        Second input array.\n",
    "    length_scale : float\n",
    "        Length scale for the kernel function.\n",
    "    signal_variance : float\n",
    "        Signal variance for the kernel function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    C : np.ndarray, shape (n_train + n_test, n_train + n_test)\n",
    "        Full covariance matrix C and its components K, K*, K**\n",
    "    \"\"\"\n",
    "    n_train = len(xy_train)\n",
    "    \n",
    "    # K(X,X)\n",
    "    K = kernel_derivative(xy_train, xy_train, length_scale, signal_variance)\n",
    "    \n",
    "    # Add the noise value 𝜎_𝑛 to the upper left block of K(X,X)\n",
    "    K[:n_train, :n_train] += (sigma_n**2) * np.eye(n_train)\n",
    "    # Add the noise value 𝜎_d to the lower right block of K(X,X)\n",
    "    K[n_train:, n_train:] += (sigma_d**2) * np.eye(n_train * 2)\n",
    "    \n",
    "    # K(X*,X)\n",
    "    K_star = kernel_derivative(xy_test, xy_train, length_scale, signal_variance)\n",
    "    \n",
    "    # K(X*,X*)\n",
    "    K_star_star = kernel_derivative(xy_test, xy_test, length_scale, signal_variance)\n",
    "    \n",
    "    # Constructing the full covariance matrix from its components\n",
    "    C = np.block([\n",
    "        [K,      K_star.T   ],\n",
    "        [K_star, K_star_star]\n",
    "        ])\n",
    "    \n",
    "    return K, K_star, K_star_star, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full covariance matrix\n",
    "C_ext = compute_ext_cov_matrix(xy_train, xy_star, length_scale, signal_variance, sigma_n, sigma_d=sigma_d)\n",
    "\n",
    "print(C_ext[3].shape)\n",
    "print(is_symmetric(C_ext[3]))\n",
    "print(is_positive_semi_definite(C_ext[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f14d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior mean and covariance using the above function\n",
    "mean, cov = compute_gpr_parameters(C_ext, y_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x3 subplot grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9, 6.5))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "for i in range(6):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Sample from posterior\n",
    "    f_star = np.random.multivariate_normal(mean=mean[:n_star].squeeze(), cov=cov[:n_star, :n_star])\n",
    "    \n",
    "    # Reshape f_star so that we can plot our data on a 2D surface\n",
    "    F_star = np.reshape(f_star, (len(x_star), -1))\n",
    "    \n",
    "    # Plot contour lines\n",
    "    contour_lines = ax.contour(X_star, Y_star, F_star, levels=levels, colors='k', linewidths=0.8)\n",
    "    ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "    \n",
    "    # Filled contours\n",
    "    contour_fill = ax.contourf(X_star, Y_star, F_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "    \n",
    "    # Axis labels and ticks\n",
    "    ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "    ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "    ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "    ax.set_title(f'Sample {i+1}', fontsize=8)\n",
    "\n",
    "fig.suptitle(\"Müller-Brown Contour Posterior Samples\", fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008e6a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x3 subplot grid\n",
    "fig, axes = plt.subplots(2, 3, figsize=(9.4, 6.4))\n",
    "axes = axes.flatten()  # Make it easier to loop over\n",
    "\n",
    "# --- Plot true potential energy surface ---\n",
    "ax = axes[0]\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, V, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, V, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Energy Surface V(x,y)', fontsize=8)\n",
    "\n",
    "# --- Plot true dx surface ---\n",
    "ax = axes[1]\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, dV_x, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, dV_x, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Gradient Surface dV/dx', fontsize=8)\n",
    "\n",
    "# --- Plot true dy surface ---\n",
    "ax = axes[2]\n",
    "# Plot contour lines\n",
    "contour_lines = ax.contour(X, Y, dV_y, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X, Y, dV_y, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Gradient Surface dV/dy', fontsize=8)\n",
    "\n",
    "# ---- Generate samples from the posterior ----\n",
    "f_star = np.random.multivariate_normal(mean=mean[:n_star].squeeze(), cov=cov[:n_star, :n_star], size=20)\n",
    "f_star = np.mean(f_star, axis=0)\n",
    "F_star = np.reshape(f_star, (len(x_star), -1))\n",
    "\n",
    "dfx_star = np.random.multivariate_normal(mean=mean[n_star:n_star*2].squeeze(), cov=cov[n_star:n_star*2, n_star:n_star*2], size=20)\n",
    "dfx_star = np.mean(dfx_star, axis=0)\n",
    "dFx_star = np.reshape(dfx_star, (len(x_star), -1))\n",
    "\n",
    "dfy_star = np.random.multivariate_normal(mean=mean[n_star*2:].squeeze(), cov=cov[n_star*2:, n_star*2:], size=20)\n",
    "dfy_star = np.mean(dfy_star, axis=0)\n",
    "dFy_star = np.reshape(dfy_star, (len(x_star), -1))\n",
    "\n",
    "# --- Plot predicted potential energy surface ---\n",
    "ax = axes[3]\n",
    "# Contour lines\n",
    "contour_lines = ax.contour(X_star, Y_star, F_star, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X_star, Y_star, F_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Predicted Energy Surface V(x,y)', fontsize=8)\n",
    "\n",
    "# --- Plot predicted dx surface ---\n",
    "ax = axes[4]\n",
    "# Contour lines\n",
    "contour_lines = ax.contour(X_star, Y_star, dFx_star, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X_star, Y_star, dFx_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Predicted Gradient Surface dV/dx', fontsize=8)\n",
    "\n",
    "# --- Plot predicted dy surface ---\n",
    "ax = axes[5]\n",
    "# Contour lines\n",
    "contour_lines = ax.contour(X_star, Y_star, dFy_star, levels=levels, colors='k', linewidths=1.0)\n",
    "ax.clabel(contour_lines, inline=True, fmt='%3.0f', fontsize=6)\n",
    "# Filled contours\n",
    "contour_fill = ax.contourf(X_star, Y_star, dFy_star, levels=levels, cmap='rainbow', extend='both', vmin=-15, vmax=8)\n",
    "# Axis labels and ticks\n",
    "ax.set_xlabel(\"x\", labelpad=2.5, fontsize=8)\n",
    "ax.set_ylabel(\"y\", labelpad=2.5, fontsize=8)\n",
    "ax.tick_params(axis='both', pad=2, labelsize=7)\n",
    "ax.set_title(f'Predicted Gradient Surface dV/dy', fontsize=8)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Leave space for suptitle\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
